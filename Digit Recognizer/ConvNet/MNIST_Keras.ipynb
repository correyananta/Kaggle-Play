{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d4c08f48-fe23-4ddb-ac46-d97f05397514",
    "_uuid": "f2156d1dd26a1243e18512002e10872c5bd7271e"
   },
   "source": [
    "```\n",
    "Image Classification : Implementation in Keras - MNIST Datasets\n",
    "Published Date: 15 October, 2018\n",
    "\n",
    "author: Mohammed Innat\n",
    "email:  innat1994@gmail.com\n",
    "website: https://iphton.github.io/iphton.github.io/\n",
    "\n",
    "\n",
    "Please feel free to use and modify this, but keep the above information. Thanks!\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# MNIST Implementation Using Convolutional Network\n",
    "\n",
    "**About Data:**\n",
    "\n",
    "The data files `train.csv` and `test.csv` contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "\n",
    "Each image is `28` pixels in height and `28` pixels in width, for a total of `784` pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "The training data set, (**train.csv**), has `785` columns. The first column, called `label`, is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "The test data set, (**test.csv**), is the same as the training set, except that it does not contain the `label` column.\n",
    "\n",
    "Read more on Kaggle: [Digit Recognizer.](https://www.kaggle.com/c/digit-recognizer/data)\n",
    "\n",
    "---\n",
    "\n",
    "**Procedure:**\n",
    "- Data Preprocessing\n",
    "- Building a Model\n",
    "- Set Hyper-Parameter\n",
    "- Evaluate the Model\n",
    "\n",
    "**Miscellaneous:**\n",
    "- Save Model and Weight \n",
    "- Tensorboard: Visualize the Computational Graph and Parameters.\n",
    "\n",
    "Finally, Create `csv` file for kaggle submission.\n",
    "\n",
    "---\n",
    "\n",
    "**Result:**\n",
    "\n",
    "I got **Final loss: 0.01153, Final accuracy: 0.99595** with implementing **ConvNet** using TensorFlow high level API **Keras** on **GeForce GTX 1050 Ti**. \n",
    "\n",
    "I set several number of epochs (20 , 50 , 100) on the training process. However, we can easily get almost **99%+** accuracy within 20 or 30 epochs. Training on a single CPU, epochs size should be set within 2 or 3~4, accuracy almost **99%**.\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin with importing packages and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit Counter:\n",
      " 1    4684\n",
      "7    4401\n",
      "3    4351\n",
      "9    4188\n",
      "2    4177\n",
      "6    4137\n",
      "0    4132\n",
      "4    4072\n",
      "8    4063\n",
      "5    3795\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "------------------------------\n",
      "Check for missing values on training set:\n",
      " count       784\n",
      "unique        1\n",
      "top       False\n",
      "freq        784\n",
      "dtype: object \n",
      "\n",
      "------------------------------\n",
      "Check for Missing values on test set:\n",
      " count       784\n",
      "unique        1\n",
      "top       False\n",
      "freq        784\n",
      "dtype: object \n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "\n",
    "# Load the data\n",
    "import pandas as pd\n",
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "test = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "Y_train = train[\"label\"]\n",
    "\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "print('Digit Counter:\\n',Y_train.value_counts() , '\\n')\n",
    "print('-'*30)\n",
    "print('Check for missing values on training set:\\n',X_train.isnull().any().describe() , '\\n')\n",
    "print('-'*30)\n",
    "print('Check for Missing values on test set:\\n',test.isnull().any().describe() , '\\n')\n",
    "print('-'*30)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "test = test / 255.0\n",
    "\n",
    "# Reshape image in 3 dimensions \n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)\n",
    "\n",
    "# On hot encoding \n",
    "from keras.utils.np_utils import to_categorical \n",
    "Y_train = to_categorical(Y_train, num_classes = 10)\n",
    "\n",
    "# Set the random seed\n",
    "random_seed = 101\n",
    "\n",
    "# Randomly split the data sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1,\n",
    "                                                  random_state=random_seed)\n",
    "\n",
    "# CNN-Keras Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "\n",
    "import keras\n",
    "adamax = keras.optimizers.Adamax(lr=0.002, beta_1=0.9, \n",
    "                                 beta_2=0.999, epsilon=None, decay=0.0)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = adamax , \n",
    "              loss = \"categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Set a learning rate annealer\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "# Using the Tensorboard callback of Keras. \n",
    "# https://stackoverflow.com/questions/42112260/how-do-i-use-the-tensorboard-callback-of-keras\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='ConvNet/KerasGraph', histogram_freq=0, \n",
    "                                         write_graph=True, write_images=True)\n",
    "\n",
    "epochs = 20 # Turn epochs to 20 ~ 50 (strong GPU) or 2 ~ 5 (strong CPU)\n",
    "batch_size = 64\n",
    "\n",
    "# Data Augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "590/590 [==============================] - 9s 15ms/step - loss: 0.6090 - acc: 0.8001 - val_loss: 0.0745 - val_acc: 0.9788\n",
      "Epoch 2/20\n",
      "590/590 [==============================] - 8s 14ms/step - loss: 0.1906 - acc: 0.9404 - val_loss: 0.1300 - val_acc: 0.9586\n",
      "Epoch 3/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.1417 - acc: 0.9557 - val_loss: 0.0395 - val_acc: 0.9881\n",
      "Epoch 4/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.1141 - acc: 0.9640 - val_loss: 0.0395 - val_acc: 0.9881\n",
      "Epoch 5/20\n",
      "590/590 [==============================] - 8s 14ms/step - loss: 0.0999 - acc: 0.9693 - val_loss: 0.0324 - val_acc: 0.9895\n",
      "Epoch 6/20\n",
      "590/590 [==============================] - 8s 14ms/step - loss: 0.0954 - acc: 0.9704 - val_loss: 0.0297 - val_acc: 0.9902\n",
      "Epoch 7/20\n",
      "590/590 [==============================] - 8s 14ms/step - loss: 0.0816 - acc: 0.9747 - val_loss: 0.0297 - val_acc: 0.9912\n",
      "Epoch 8/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.0761 - acc: 0.9766 - val_loss: 0.0255 - val_acc: 0.9912\n",
      "Epoch 9/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.0739 - acc: 0.9772 - val_loss: 0.0222 - val_acc: 0.9924\n",
      "Epoch 10/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.0704 - acc: 0.9780 - val_loss: 0.0191 - val_acc: 0.9945\n",
      "Epoch 11/20\n",
      "590/590 [==============================] - 8s 14ms/step - loss: 0.0636 - acc: 0.9810 - val_loss: 0.0232 - val_acc: 0.9933\n",
      "Epoch 12/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.0615 - acc: 0.9815 - val_loss: 0.0203 - val_acc: 0.9943\n",
      "Epoch 13/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.0572 - acc: 0.9823 - val_loss: 0.0169 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 14/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.0520 - acc: 0.9844 - val_loss: 0.0150 - val_acc: 0.9952\n",
      "Epoch 15/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.0512 - acc: 0.9851 - val_loss: 0.0180 - val_acc: 0.9940\n",
      "Epoch 16/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.0470 - acc: 0.9856 - val_loss: 0.0133 - val_acc: 0.9940\n",
      "Epoch 17/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.0466 - acc: 0.9858 - val_loss: 0.0159 - val_acc: 0.9938\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 18/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.0421 - acc: 0.9866 - val_loss: 0.0117 - val_acc: 0.9960\n",
      "Epoch 19/20\n",
      "590/590 [==============================] - 8s 13ms/step - loss: 0.0424 - acc: 0.9867 - val_loss: 0.0124 - val_acc: 0.9955\n",
      "Epoch 20/20\n",
      "590/590 [==============================] - 8s 14ms/step - loss: 0.0417 - acc: 0.9877 - val_loss: 0.0111 - val_acc: 0.9960\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit the model\n",
    "trained = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 1, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.01115, Final accuracy: 0.99595\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print(\"Final loss: {0:.5f}, Final accuracy: {1:.5f}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at confusion matrix \n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display error results \n",
    "# Errors are difference between predicted labels and true labels\n",
    "\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_val[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2 \n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    \n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model and Weights\n",
    "We can save the model in `json` and weights in a `hdf5` file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model Into JSON.\n",
      "Saving Trained Model Weights Into HDF5.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "# the keras model which is trained is defined as 'model' in this example\n",
    "model_json = model.to_json()\n",
    "print('Saving Model Into JSON.')\n",
    "with open(\"ConvNet/KerasTrained_Model/trained_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# saving weights to HDF5\n",
    "print('Saving Trained Model Weights Into HDF5.')\n",
    "model.save_weights('ConvNet/KerasTrained_Model/trained_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model and Weights\n",
    "To use the same trained model for further testing we can simply load the `hdf5` file and use it for the prediction of different data. To load the **weights**, we need first to build our **model** which we've already done.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('ConvNet/KerasTrained_Model/trained_model.json', 'r')\n",
    "\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"ConvNet/KerasTrained_Model/trained_model_weights.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Reloaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.01115, Final accuracy: 0.99595\n"
     ]
    }
   ],
   "source": [
    "loaded_model.compile(optimizer = adamax , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "loss, acc = loaded_model.evaluate(X_val, Y_val, verbose=0)\n",
    "\n",
    "print(\"Final loss: {0:.5f}, Final accuracy: {1:.5f}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Save Model + Weight Together - Pretrainede Model\n",
    "Another saving technique is `model.save(filepath)`. This save function saves:\n",
    "\n",
    "- The architecture of the model, allowing to re-create the model.\n",
    "- The weights of the model.\n",
    "- The training configuration (loss, optimizer).\n",
    "- The state of the optimizer, allowing to resume training exactly where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk.\n"
     ]
    }
   ],
   "source": [
    "# Same Model and Weight at a same time - no need to compile as we previously did\n",
    "# where we manually save model and weight and loaded them separately.\n",
    "model.save('ConvNet/KerasTrained_Model/pretrained_model.h5')\n",
    "from keras.models import load_model\n",
    "loaded_model = load_model('ConvNet/KerasTrained_Model/pretrained_model.h5')\n",
    "print('Loaded model from disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.01115, Final accuracy: 0.99595\n"
     ]
    }
   ],
   "source": [
    "loss, acc = loaded_model.evaluate(X_val, Y_val, verbose=0)\n",
    "print(\"Final loss: {0:.5f}, Final accuracy: {1:.5f}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission \n",
    "Create a submission file for kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(test)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results, axis = 1)\n",
    "results = pd.Series(results, name=\"Label\")\n",
    "\n",
    "submission = pd.concat([pd.Series(range(1,28001), name = \"ImageId\"),results],axis = 1)\n",
    "submission.to_csv(\"ConvNet/submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
